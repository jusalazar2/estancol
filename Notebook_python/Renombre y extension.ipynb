{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Se encontraron 9 imágenes. Renombrando...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Renombrado completado con éxito.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def seleccionar_carpeta():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    carpeta = filedialog.askdirectory(title=\"Selecciona la carpeta con las imágenes\")\n",
    "    return carpeta\n",
    "\n",
    "def renombrar_imagenes(carpeta):\n",
    "    if not os.path.isdir(carpeta):\n",
    "        display(Markdown(\"**Error:** La carpeta seleccionada no existe.\"))\n",
    "        return\n",
    "    \n",
    "    imagenes = [f for f in os.listdir(carpeta) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\"))]\n",
    "    imagenes.sort()\n",
    "    \n",
    "    if not imagenes:\n",
    "        display(Markdown(\"**No se encontraron imágenes en la carpeta seleccionada.**\"))\n",
    "        return\n",
    "    \n",
    "    display(Markdown(f\"**Se encontraron {len(imagenes)} imágenes. Renombrando...**\"))\n",
    "    \n",
    "    for i, imagen in enumerate(imagenes, start=1):\n",
    "        extension = os.path.splitext(imagen)[1]\n",
    "        nuevo_nombre = f\"imagen{i}{extension}\"\n",
    "        ruta_actual = os.path.join(carpeta, imagen)\n",
    "        nueva_ruta = os.path.join(carpeta, nuevo_nombre)\n",
    "        os.rename(ruta_actual, nueva_ruta)\n",
    "    \n",
    "    display(Markdown(\"**Renombrado completado con éxito.**\"))\n",
    "\n",
    "# Ejecutar el proceso\n",
    "carpeta_seleccionada = seleccionar_carpeta()\n",
    "if carpeta_seleccionada:\n",
    "    renombrar_imagenes(carpeta_seleccionada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renombrado: imagen10.JPG → imagen10.jpg\n",
      "Renombrado: imagen11.JPG → imagen11.jpg\n",
      "Renombrado: imagen12.JPG → imagen12.jpg\n",
      "Renombrado: imagen13.JPG → imagen13.jpg\n",
      "Renombrado: imagen14.JPG → imagen14.jpg\n",
      "Renombrado: imagen15.JPG → imagen15.jpg\n",
      "Renombrado: imagen16.JPG → imagen16.jpg\n",
      "Renombrado: imagen17.JPG → imagen17.jpg\n",
      "Renombrado: imagen18.JPG → imagen18.jpg\n",
      "Renombrado: imagen19.JPG → imagen19.jpg\n",
      "Renombrado: imagen2.JPG → imagen2.jpg\n",
      "Renombrado: imagen20.JPG → imagen20.jpg\n",
      "Renombrado: imagen3.JPG → imagen3.jpg\n",
      "Renombrado: imagen4.JPG → imagen4.jpg\n",
      "Renombrado: imagen5.JPG → imagen5.jpg\n",
      "Renombrado: imagen6.JPG → imagen6.jpg\n",
      "Renombrado: imagen7.JPG → imagen7.jpg\n",
      "Renombrado: imagen8.JPG → imagen8.jpg\n",
      "Renombrado: imagen9.JPG → imagen9.jpg\n",
      "\n",
      "✅ 19 archivo(s) renombrado(s).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Cambiar esta ruta a tu carpeta de imágenes\n",
    "carpeta = r'C:\\Users\\pc\\Videos\\estancol\\productos\\static\\images\\Estanteria-super-pesada\\proyectos\\2_world_cargo_group'\n",
    "\n",
    "# 2. Extensiones de imágenes conocidas\n",
    "extensiones_imagenes = {'.JPG', '.JPEG', '.PNG', '.WEBP', '.GIF', '.TIFF', '.BMP'}\n",
    "\n",
    "# 3. Renombrar extensiones a minúsculas\n",
    "renombrados = 0\n",
    "for archivo in os.listdir(carpeta):\n",
    "    ruta_completa = os.path.join(carpeta, archivo)\n",
    "    if os.path.isfile(ruta_completa):\n",
    "        nombre, extension = os.path.splitext(archivo)\n",
    "        if extension.upper() in extensiones_imagenes and extension != extension.lower():\n",
    "            nuevo_nombre = nombre + extension.lower()\n",
    "            nueva_ruta = os.path.join(carpeta, nuevo_nombre)\n",
    "            os.rename(ruta_completa, nueva_ruta)\n",
    "            print(f'Renombrado: {archivo} → {nuevo_nombre}')\n",
    "            renombrados += 1\n",
    "\n",
    "print(f'\\n✅ {renombrados} archivo(s) renombrado(s).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Escoge el país:\n",
      "    1. Argentina\n",
      "    2. Bolivia\n",
      "    3. Brasil\n",
      "    4. Chile\n",
      "    5. Colombia\n",
      "    6. Costa Rica\n",
      "    7. Dominicana\n",
      "    8. Ecuador\n",
      "    9. Guatemala\n",
      "    10. Honduras\n",
      "    11. México\n",
      "    12. Nicaragua\n",
      "    13. Panamá\n",
      "    14. Paraguay\n",
      "    15. Perú\n",
      "    16. Salvador\n",
      "    17. Uruguay\n",
      "    18. Venezuela\n",
      "        \n",
      "\n",
      "Scrapeando pagina numero 1. https://listado.mercadolibre.com.co/motorola\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m s \u001b[38;5;241m=\u001b[39m Scraper()\n\u001b[0;32m    138\u001b[0m s\u001b[38;5;241m.\u001b[39mmenu()\n\u001b[1;32m--> 139\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscraping\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m s\u001b[38;5;241m.\u001b[39mexport_to_csv()\n",
      "Cell \u001b[1;32mIn[11], line 106\u001b[0m, in \u001b[0;36mScraper.scraping\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# iteration to scrape posts\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# get the title\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[43mpost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# get the price\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     price \u001b[38;5;241m=\u001b[39m post\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mandes-money-amount__fraction\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class Scraper():\n",
    "\n",
    "    def menu(self):\n",
    "        menu = (\"\"\"\n",
    "    Escoge el país:\n",
    "    1. Argentina\n",
    "    2. Bolivia\n",
    "    3. Brasil\n",
    "    4. Chile\n",
    "    5. Colombia\n",
    "    6. Costa Rica\n",
    "    7. Dominicana\n",
    "    8. Ecuador\n",
    "    9. Guatemala\n",
    "    10. Honduras\n",
    "    11. México\n",
    "    12. Nicaragua\n",
    "    13. Panamá\n",
    "    14. Paraguay\n",
    "    15. Perú\n",
    "    16. Salvador\n",
    "    17. Uruguay\n",
    "    18. Venezuela\n",
    "        \"\"\")\n",
    "\n",
    "        valid_options = list(range(1, 19))\n",
    "\n",
    "        while True:\n",
    "            print(menu)\n",
    "            opcion = int(input('Número de país (Ejemplo: 5): '))\n",
    "\n",
    "            if opcion in valid_options:\n",
    "\n",
    "                urls = {\n",
    "                1: 'https://listado.mercadolibre.com.ar/',\n",
    "                2: 'https://listado.mercadolibre.com.bo/',\n",
    "                3: 'https://listado.mercadolibre.com.br/',\n",
    "                4: 'https://listado.mercadolibre.cl/',\n",
    "                5: 'https://listado.mercadolibre.com.co/',\n",
    "                6: 'https://listado.mercadolibre.com.cr/',\n",
    "                7: 'https://listado.mercadolibre.com.do/',\n",
    "                8: 'https://listado.mercadolibre.com.ec/',\n",
    "                9: 'https://listado.mercadolibre.com.gt/',\n",
    "                10: 'https://listado.mercadolibre.com.hn/',\n",
    "                11: 'https://listado.mercadolibre.com.mx/',\n",
    "                12: 'https://listado.mercadolibre.com.ni/',\n",
    "                13: 'https://listado.mercadolibre.com.pa/',\n",
    "                14: 'https://listado.mercadolibre.com.py/',\n",
    "                15: 'https://listado.mercadolibre.com.pe/',\n",
    "                16: 'https://listado.mercadolibre.com.sv/',\n",
    "                17: 'https://listado.mercadolibre.com.uy/',\n",
    "                18: 'https://listado.mercadolibre.com.ve/',\n",
    "                }\n",
    "\n",
    "                self.base_url = urls[opcion]\n",
    "                break\n",
    "            else:\n",
    "                print(\"Escoge un número del 1 al 18\")\n",
    "\n",
    "\n",
    "\n",
    "    def scraping(self):\n",
    "        # User search\n",
    "        product_name = input(\"\\nProducto: \")\n",
    "        # Clean the user input\n",
    "        cleaned_name = product_name.replace(\" \", \"-\").lower()\n",
    "        # Create the urls to scrap\n",
    "        urls = [self.base_url + cleaned_name]\n",
    "\n",
    "        page_number = 50\n",
    "        for i in range(0, 10000, 50):\n",
    "            urls.append(f\"{self.base_url}{cleaned_name}_Desde_{page_number + 1}_NoIndex_True\")\n",
    "            page_number += 50\n",
    "\n",
    "        # create a list to save the data\n",
    "        self.data = []\n",
    "        # create counter\n",
    "        c = 1\n",
    "            \n",
    "        # Iterate over each url\n",
    "        for i, url in enumerate(urls, start=1):\n",
    "\n",
    "            # Get the html of the page\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "            # take all posts\n",
    "            content = soup.find_all('li', class_='ui-search-layout__item')\n",
    "            \n",
    "            # Check if there's no content to scrape\n",
    "            if not content:\n",
    "                print(\"\\nTermino el scraping.\")\n",
    "                break\n",
    "\n",
    "            print(f\"\\nScrapeando pagina numero {i}. {url}\")\n",
    "            \n",
    "            \n",
    "            # iteration to scrape posts\n",
    "            for post in content:\n",
    "                # get the title\n",
    "                title = post.find('h2').text\n",
    "                # get the price\n",
    "                price = post.find('span', class_='andes-money-amount__fraction').text\n",
    "                # get the url post\n",
    "                post_link = post.find(\"a\")[\"href\"]\n",
    "                # get the url image\n",
    "                try:\n",
    "                    img_link = post.find(\"img\")[\"data-src\"]\n",
    "                except:\n",
    "                    img_link = post.find(\"img\")[\"src\"]\n",
    "                \n",
    "                # show the data already scraped\n",
    "                # print(f\"{c}. {title}, {price}, {post_link}, {img_link}\")\n",
    "\n",
    "                # save in a dictionary\n",
    "                post_data = {\n",
    "                    \"title\": title,\n",
    "                    \"price\": price,\n",
    "                    \"post link\": post_link,\n",
    "                    \"image link\": img_link            \n",
    "                }\n",
    "                # save the dictionaries in a list\n",
    "                self.data.append(post_data)\n",
    "                c += 1\n",
    "\n",
    "    def export_to_csv(self):\n",
    "        # export to a csv file\n",
    "        df = pd.DataFrame(self.data)\n",
    "        df.to_csv(r\"data/mercadolibre_scraped_data.csv\", sep=\";\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    s = Scraper()\n",
    "    s.menu()\n",
    "    s.scraping()\n",
    "    s.export_to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
